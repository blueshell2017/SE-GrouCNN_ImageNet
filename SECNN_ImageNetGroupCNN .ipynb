{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/localadmin/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# Impor tensorflow and numpy\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import math as m\n",
    "import time\n",
    "# For validation\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import tensorpack.dataflow\n",
    "# For plotting\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# Add the library to the system path\n",
    "import os,sys\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'\n",
    "se2cnn_source =  os.path.join(os.getcwd(),'..')\n",
    "if se2cnn_source not in sys.path:\n",
    "    sys.path.append(se2cnn_source)\n",
    "\n",
    "# Import the library\n",
    "import se2cnn.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorpack import logger, QueueInput\n",
    "from tensorpack.models import *\n",
    "from tensorpack.callbacks import *\n",
    "from tensorpack.train import (\n",
    "    TrainConfig, SyncMultiGPUTrainerReplicated, launch_train_with_config)\n",
    "from tensorpack.dataflow import FakeData\n",
    "from tensorpack.tfutils import argscope, get_model_loader\n",
    "from tensorpack.utils.gpu import get_num_gpu\n",
    "from imagenet_utils import (\n",
    "    fbresnet_augmentor, get_imagenet_dataflow, ImageNetModel,\n",
    "    eval_on_ILSVRC12)\n",
    "from tensorpack import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xavier's/He-Rang-Zhen-Sun initialization for layers that are followed ReLU\n",
    "def weight_initializer(n_in, n_out):\n",
    "    return tf.random_normal_initializer(mean=0.0, stddev=m.sqrt(2.0 / (n_in))\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def size_of(tensor) :\n",
    "    # Multiply elements one by one\n",
    "    result = 1\n",
    "    for x in tensor.get_shape().as_list():\n",
    "         result = result * x \n",
    "    return result "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load imagenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image_net_path = \"/mnt/nas2/gx/ImageNet\"\n",
    "batch = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(name, batch):\n",
    "    isTrain = name == 'train'\n",
    "    augmentors = fbresnet_augmentor(isTrain)\n",
    "    return get_imagenet_dataflow(\n",
    "        Image_net_path, name, batch, augmentors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "# Part 3: Build a graph (design the G-CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "graph = tf.Graph()\n",
    "graph.as_default()\n",
    "tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ntheta = 1 # Kernel size in angular direction\n",
    "Nxy=5       # Kernel size in spatial direction\n",
    "Nc = 4      # Number of channels in the initial layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs_ph = tf.placeholder( dtype = tf.float32, shape = [None,224,224,3] )\n",
    "labels_ph = tf.placeholder( dtype = tf.int32, shape = [None,] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_in = inputs_ph\n",
    "Nc_in = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernels={}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z2-SE2N BASE KERNEL SHAPE: (5, 5, 3, 4)\n",
      "Z2-SE2N ROTATED KERNEL SET SHAPE: (1, 5, 5, 3, 4)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, ?, ?, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(1)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = Nc\n",
    "\n",
    "    ## Perform lifting convolution\n",
    "    # The kernels used in the lifting layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Lifting layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.z2_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw,\n",
    "                            orientations_nb = Ntheta)\n",
    "    # Add bias\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    \n",
    "    ## Perform (spatial) max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1), Dimension(4)])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Layer 2: SE2-conv, max-pool, relu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE2N-SE2N BASE KERNEL SHAPE: (5, 5, 1, 4, 4)\n",
      "SE2N-SE2N ROTATED KERNEL SET SHAPE: (1, 5, 5, 1, 4, 4)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, ?, ?, 1, 4)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(2)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = Nc\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # The group convolution layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw)\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE2N-SE2N BASE KERNEL SHAPE: (6, 6, 1, 4, 8)\n",
      "SE2N-SE2N ROTATED KERNEL SET SHAPE: (1, 6, 6, 1, 4, 8)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, ?, ?, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(3)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 2*Nc\n",
    "    Nxy = 6\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # The group convolution layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw)\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1), Dimension(8)])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE2N-SE2N BASE KERNEL SHAPE: (5, 5, 1, 8, 8)\n",
      "SE2N-SE2N ROTATED KERNEL SET SHAPE: (1, 5, 5, 1, 8, 8)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, ?, ?, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(4)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 2*Nc\n",
    "    Nxy = 5\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # The group convolution layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw)\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1), Dimension(8)])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SE2N-SE2N BASE KERNEL SHAPE: (5, 5, 1, 8, 8)\n",
      "SE2N-SE2N ROTATED KERNEL SET SHAPE: (1, 5, 5, 1, 8, 8)\n",
      "OUTPUT SE2N ACTIVATIONS SHAPE: (?, ?, ?, 1, 8)\n"
     ]
    }
   ],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(5)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 2*Nc\n",
    "    Nxy = 5\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Ntheta,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Ntheta*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # The group convolution layer\n",
    "    tensor_out, kernels_formatted = se2cnn.layers.se2n_se2n(\n",
    "                            input_tensor = tensor_in,\n",
    "                            kernel = kernels_raw)\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Perform max-pooling\n",
    "    tensor_out = se2cnn.layers.spatial_max_pool( input_tensor=tensor_out, nbOrientations=Ntheta)\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(1), Dimension(8)])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the orientation and channel dimension\n",
    "tensor_in = tf.concat([tensor_in[:,:,:,i,:] for i in range(Ntheta)],3)\n",
    "Nc_in = tensor_in.get_shape().as_list()[-1]\n",
    "\n",
    "# 2D convolution layer\n",
    "with tf.variable_scope(\"Layer_{}\".format(6)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 4*Nc\n",
    "    Nxy = 3\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [Nxy,Nxy,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(Nxy*Nxy*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(16)])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D convolution layer\n",
    "with tf.variable_scope(\"Layer_{}\".format(7)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 128\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [1,1,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "    # Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## Apply ReLU\n",
    "    tensor_out = tf.nn.relu(tensor_out)\n",
    "\n",
    "    ## Prepare for the next layer\n",
    "    tensor_in = tensor_out\n",
    "    Nc_in = Nc_out\n",
    "    \n",
    "    ## Save kernels for inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(None), Dimension(None), Dimension(128)])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_in.get_shape()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.variable_scope(\"Layer_{}\".format(8)) as _scope:\n",
    "    ## Settings\n",
    "    Nc_out = 1000\n",
    "\n",
    "    ## Perform group convolution\n",
    "    # The kernels used in the group convolution layer\n",
    "    kernels_raw = tf.get_variable(\n",
    "                        'kernel', \n",
    "                        [1,1,Nc_in,Nc_out],\n",
    "                        initializer=weight_initializer(1*1*Nc_in,Nc_out))\n",
    "    tf.add_to_collection('raw_kernels', kernels_raw)\n",
    "    bias = tf.get_variable( # Same bias for all orientations\n",
    "                        \"bias\",\n",
    "                        [1, 1, 1, Nc_out], \n",
    "                        initializer=tf.constant_initializer(value=0.01))\n",
    "\n",
    "    \n",
    "    ## Convolution layer\n",
    "    tensor_out = tf.nn.conv2d(\n",
    "                        input = tensor_in,\n",
    "                        filter=kernels_raw,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding=\"VALID\")\n",
    "    tensor_out = tensor_out + bias\n",
    "    \n",
    "    ## The output logits\n",
    "    logits = tensor_out[:,0,0,:]\n",
    "    predictions = tf.argmax(input=logits, axis=1)\n",
    "    probabilities = tf.nn.softmax(logits)\n",
    "    \n",
    "    ## Save the kernels for later inspection\n",
    "    kernels[_scope.name] = kernels_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(1000)])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.get_shape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the loss and the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss\n",
    "loss = tf.losses.sparse_softmax_cross_entropy(labels=labels_ph, logits=logits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----\n",
      "RAW kernel shapes:\n",
      "[Layer_1/kernel:0]: (5, 5, 3, 4), total nr of weights = 300\n",
      "[Layer_2/kernel:0]: (5, 5, 1, 4, 4), total nr of weights = 400\n",
      "[Layer_3/kernel:0]: (6, 6, 1, 4, 8), total nr of weights = 1152\n",
      "[Layer_4/kernel:0]: (5, 5, 1, 8, 8), total nr of weights = 1600\n",
      "[Layer_5/kernel:0]: (5, 5, 1, 8, 8), total nr of weights = 1600\n",
      "[Layer_6/kernel:0]: (3, 3, 8, 16), total nr of weights = 1152\n",
      "[Layer_7/kernel:0]: (1, 1, 16, 128), total nr of weights = 2048\n",
      "[Layer_8/kernel:0]: (1, 1, 128, 1000), total nr of weights = 128000\n",
      "-----\n"
     ]
    }
   ],
   "source": [
    "#-- Define the l2 loss \n",
    "weightDecay=5e-4\n",
    "# Get the raw kernels\n",
    "variables_wd = tf.get_collection('raw_kernels')\n",
    "print('-----')\n",
    "print('RAW kernel shapes:')\n",
    "for v in variables_wd: print( \"[{}]: {}, total nr of weights = {}\".format(v.name, v.get_shape(), size_of(v)))\n",
    "print('-----')\n",
    "loss_l2 = weightDecay*sum([tf.nn.l2_loss(ker) for ker in variables_wd])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure the Training Op (for TRAIN mode)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "\n",
    "train_op = optimizer.minimize(\n",
    "    loss=loss + loss_l2,\n",
    "    global_step=tf.train.get_global_step())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#-- Start the (GPU) session\n",
    "initializer = tf.global_variables_initializer()\n",
    "session = tf.Session(graph=tf.get_default_graph()) #-- Session created\n",
    "session.run(initializer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs=10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0909 19:04:10 @fs.py:100]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Env var $TENSORPACK_DATASET not set, using /home/localadmin/tensorpack_data for datasets.\n",
      "\u001b[32m[0909 19:04:11 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0909 19:04:11 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 19:04:11 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  0  finished... Average loss =  6.7654 , time =  6561.3937\n",
      "\u001b[32m[0909 20:53:34 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0909 20:53:34 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 20:53:34 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  1  finished... Average loss =  6.1936 , time =  6551.4874\n",
      "\u001b[32m[0909 22:42:47 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0909 22:42:47 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0909 22:42:47 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  2  finished... Average loss =  6.0758 , time =  6865.458\n",
      "\u001b[32m[0910 00:37:14 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 00:37:14 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 00:37:15 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  3  finished... Average loss =  6.0389 , time =  6950.5637\n",
      "\u001b[32m[0910 02:33:06 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 02:33:06 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 02:33:07 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  4  finished... Average loss =  6.0081 , time =  6916.9062\n",
      "\u001b[32m[0910 04:28:25 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 04:28:25 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 04:28:26 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  5  finished... Average loss =  5.9835 , time =  6900.4563\n",
      "\u001b[32m[0910 06:23:28 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 06:23:28 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:28 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:29 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 06:23:29 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  6  finished... Average loss =  5.965 , time =  6930.3331\n",
      "\u001b[32m[0910 08:19:00 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 08:19:00 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 08:19:01 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  7  finished... Average loss =  5.9513 , time =  7009.7857\n",
      "\u001b[32m[0910 10:15:52 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 10:15:52 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 10:15:53 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  8  finished... Average loss =  5.9374 , time =  7060.0196\n",
      "\u001b[32m[0910 12:13:34 @imagenet_utils.py:104]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m DataFlow may become the bottleneck when too few processes are used.\n",
      "\u001b[32m[0910 12:13:34 @parallel.py:291]\u001b[0m [PrefetchDataZMQ] Will fork a dataflow more than one times. This assumes the datapoints are i.i.d.\n",
      "PrefetchDataZMQ successfully cleaned-up.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "\u001b[32m[0910 12:13:35 @argtools.py:152]\u001b[0m \u001b[5m\u001b[31mWRN\u001b[0m Install python-prctl so that processes can be cleaned with guarantee.\n",
      "Epoch  9  finished... Average loss =  5.925 , time =  7060.442\n"
     ]
    }
   ],
   "source": [
    "for epoch_nr in range(n_epochs):\n",
    "    loss_sum = 0\n",
    "    NItPerEpoch = 0\n",
    "    df = get_data('train',batch)\n",
    "    df.reset_state()\n",
    "    generator = df.get_data()\n",
    "    tStart = time.time()\n",
    "    for myiter in generator:\n",
    "        NItPerEpoch=NItPerEpoch+1\n",
    "        feed_dict = {\n",
    "                inputs_ph: myiter[0],\n",
    "                labels_ph: myiter[1]\n",
    "                }\n",
    "        operators_output = session.run([ loss , train_op ], feed_dict)\n",
    "        loss_sum += operators_output[0]\n",
    "    loss_average = loss_sum/NItPerEpoch\n",
    "    tElapsed = time.time() - tStart\n",
    "    print('Epoch ' , epoch_nr , ' finished... Average loss = ' , round(loss_average,4) , ', time = ',round(tElapsed,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
